<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>分布式 on Huija&#39;s Blog</title>
    <link>https://huija.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/</link>
    <description>Recent content in 分布式 on Huija&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 19 May 2021 13:43:19 +0800</lastBuildDate><atom:link href="https://huija.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MongoDB分片集</title>
      <link>https://huija.github.io/mongodb-shardset/</link>
      <pubDate>Wed, 19 May 2021 13:43:19 +0800</pubDate>
      
      <guid>https://huija.github.io/mongodb-shardset/</guid>
      <description>MongoDB分片集, 能够应对普通复制集无法应对的海量数据场景, 以及地理数据分布场景.
分片集是基于复制集基础上的, 从架构角度, 复制集可以平滑地过度到分片集, 而无需修改代码, 但是简单地这么思考, 其实并不正确, 分片集群, 相比复制集有得也有失, 是有所掣肘的.
1. 浅谈分片集 先撇开MongoDB谈分片
普通的RDBMS如MySQL和PostgreSQL, 基本没提供分片功能, 当然, 他们自己甚至没有提供复制集failover的功能, 但是也有人基于他们做了一些分片产品, 比如MyCat给MySQL分片, 基于PostgreSQL的GreenPlum等, 也可以看出, 对于RDBMS本身, 分片不是产品本身核心关心的点. Redis提供了分片模式, 也就是他们的Cluster集群模式, 总共16384个slot数据槽, 可以通过手动分配这些槽在指定的分片上, 而数据在哪个槽内, 则是固定的一致性hash算法来实现的,其分片集群的作用, 主要是解决, 数据过多, 单机内存不够的问题. 消息队列, Kafka的分区Partition也是分片思想, 主要是用于多消费者, 增大吞吐量. 大数据, HDFS的分块, HBase的Region等 1.1. 分片集的意义 分片集核心的意义是横向拓展, 当一台机器性能不够时, 通过增加机器, 来提高系统整体能够提供的服务量.
传统提高性能的方法, 区分为纵向拓展(垂直拓展)和横向拓展(水平拓展), 前者是提升单机性能, 后者是增加机器.
1.2. 分片集的问题 分片集最核心的问题, 实际就是分片的策略问题, 容易出现的问题如下.
数据过于集中于一个分片, 也就是热点问题, 分了个寂寞. 数据过于分散, 一个范围查询, 需要查到所有分片, 才能把数据查出来. 次要的问题: 数据迁移等
2. MongoDB分片集 MongoDB的分片集基于复制集而来, 每一个数据复制集, 都可以理解为一个分片Shard, 存放部分数据, 当复制集升到分片集后, Client就不该直接访问这些复制集群了, 而是访问更上层的mongos集群, 其负责请求的转发, 是Client与Shard之间的代理者, 此外, 还有一个Config Servers复制集群, 用于存储分片集群的元数据,分片配置等信息.</description>
    </item>
    
    <item>
      <title>MongoDB复制集</title>
      <link>https://huija.github.io/mongodb-replicaset/</link>
      <pubDate>Fri, 07 May 2021 13:37:16 +0800</pubDate>
      
      <guid>https://huija.github.io/mongodb-replicaset/</guid>
      <description>MongoDB复制集是MongoDB分布式系统的核心.
mongodb的api对于单点,复制集,分片集群都通用的, 这也就意味着, 改变底层数据库的部署架构, 客户端代码基本无需改动.
1. 浅谈复制集 先撇开MongoDB单独来讲复制集群, 顾名思义, 就是从原先的机器复制一份, 一起部署为一个集群, 在分布式场景是很常见的部署架构, 以数据库们为例, 就有MySQL基于binlog的主从部署, PostgreSQL基于WAL日志的同步/异步流复制, Redis的主从复制等.
1.1. 复制集的意义 复制集本身, 解决的根本问题是, 单机部署的单点故障问题, 当主节点挂掉, 可以转正从节点, 也就是数据冗余备份以及高可用. 衍生出来的, 是可以进行读写分流, 并可以针对读的流量进行横向扩展, 也就是可伸缩性与负载均衡. 1.2. 复制集的问题 复制集的最核心的问题集中于数据消息从主节点到从节点的备份策略, 同步还是异步
如果同步备份, 那么会保证每个操作后, 主从的数据都是一致的, 也就是强一致性, 但是如果从节点挂了, 使用同步的策略, 那主节点的请求则需要返回失败, 这就涉及集群可用性的问题. 一般数据库的主流策略都是异步, 不追求数据的强一致性, 而是最终一致性, 重点保证可用性, 而实时性问题, 则通过一定手段进行避免. 次要的问题: failover障碍转移, 主节点选举, 数据消息的协议等.
2. MongoDB复制集 MongoDB的主从叫Primary和Secondary, 其复制集由一个Primary和多个Secondary组成, Primary可以进行读写操作, Secondary回放Primary发送的oplog, 进行数据备份.
还有一种节点类型, 叫Arbiter, 仲裁者, 其主要负责选举投票, 而不进行数据备份, 也不能竞选Primary.
2.1. 典型的三节点部署 MogoDB主节点的选举条件是需要半数以上的投票节点投票, 一般节点数量都会设置为单数, 也就是2n+1, 该集群最多支持挂掉n个节点, 集群仍然能选出主节点并正常工作, 与2(n+1)个节点能够容忍的数量是一样的, 所以一般节点数(具有投票权)都是单数</description>
    </item>
    
  </channel>
</rss>
